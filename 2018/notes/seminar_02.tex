\documentclass[12pt]{article} % размер шрифта

\usepackage{tikz} % картинки в tikz
\usepackage{microtype} % свешивание пунктуации

\usepackage{array} % для столбцов фиксированной ширины

\usepackage{url} % для вставки ссылок \url{...}

\usepackage{indentfirst} % отступ в первом параграфе

\usepackage{sectsty} % для центрирования названий частей
\allsectionsfont{\centering} % приказываем центрировать все sections

\usepackage{amsthm} % теоремы и доказательства
\usepackage{amssymb}
\theoremstyle{definition} % прямой шрифт в условии теорем
\newtheorem{theorem}{Теорема}[section]

\usepackage{hyperref}
\usepackage{amsmath} % куча стандартных математических плюшек

\usepackage[top=2cm, left=1.5cm, right=1.5cm, bottom=2cm]{geometry} % размер текста на странице

\usepackage{lastpage} % чтобы узнать номер последней страницы

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке
\usepackage{caption} % подписи к картинкам без плавающего окружения figure


\usepackage{fancyhdr} % весёлые колонтитулы
\pagestyle{fancy}
\lhead{Эконометрика, финтех}
\chead{}
\rhead{2018-09-29, встреча 2}
\lfoot{}
\cfoot{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}



\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет картина Последний день Помпеи}
% команда \listoftodos — печатает все поставленные \todo'шки

\usepackage{booktabs} % красивые таблицы
% заповеди из документации:
% 1. Не используйте вертикальные линии
% 2. Не используйте двойные линии
% 3. Единицы измерения помещайте в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"

\usepackage{fontspec} % поддержка разных шрифтов
\usepackage{polyglossia} % поддержка разных языков

\setmainlanguage{russian}
\setotherlanguages{english}

\setmainfont{Linux Libertine O} % выбираем шрифт
% если Linux Libertine не установлен, то
% можно также попробовать Helvetica, Arial, Cambria и т.Д.

% чтобы использовать шрифт Linux Libertine на личном компе,
% его надо предварительно скачать по ссылке
% http://www.linuxlibertine.org/index.php?id=91&L=1

% на сервисах типа sharelatex.com этот шрифт есть :)

\newfontfamily{\cyrillicfonttt}{Linux Libertine O}
% пояснение зачем нужно шаманство с \newfontfamily
% http://tex.stackexchange.com/questions/91507/

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
\setlist[enumerate, 2]{label=\asbuk*),ref=\asbuk*} % списки уровня 2 будут буквами а) б) ...

%% эконометрические и вероятностные сокращения
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{E}
\def \hb{\hat{\beta}}
\def \hs{\hat{\sigma}}
\def \htheta{\hat{\theta}}
\def \s{\sigma}
\def \hy{\hat{y}}
\def \hY{\hat{Y}}
\def \v1{\vec{1}}
\def \e{\varepsilon}
\def \he{\hat{\e}}
\def \z{z}
\def \hVar{\widehat{\Var}}
\def \hCorr{\widehat{\Corr}}
\def \hCov{\widehat{\Cov}}
\def \cN{\mathcal{N}}


\begin{document}

Конспектировала: Ермолова Марина.

\section{Задачи из домашнего задания}

\subsection{Задача \href{https://github.com/bdemeshev/metrics_pro/raw/master/metrics_pro.pdf}{$1.13$}}
Вспомним определения:
\[
TSS = \sum_i (y_i - \overline{y})^2
\]
\[
RSS = \sum_i (y_i - \hy_i)^2
\]
\[
ESS = \sum_i (\hy_i - \overline{y})^2
\]
\begin{enumerate}
\item
Преположим, что после добавления точки к регрессии величина $RSS$ уменьшится. Тогда получается, что \[\sum_{i=1}^n (y_i - \hy_i^*)^2 \leq \sum_{i=1}^{n+1} (y_i - \hy_i^*)^2 < \sum_{i=1}^{n} (y_i - \hy_i)^2\] Из этого следует, что новая регрессия обеспечивает меньшую сумму квадратов, а значит исходная регрессия не была оптимальной. Противоречие.
\[
RSS \leq RSS^*
\]
\item
Проведем рассуждения, аналогичные прошлому пункту:
Допустим, $TSS \leq TSS^*$
\[\sum_{i=1}^n (y_i - \overline{y}^*)^2 \leq \sum_{i=1}^{n+1} (y_i - \overline{y}_i^*)^2 < \sum_{i=1}^{n} (y_i - \overline{y}_i)^2
\]
Известно, что если проецировать любой вектор на $\vec{1}$, то получается $\overline{y}\vec{1}$. Но мы знаем, что при решении задачи минимизации
\\
\[\min_{a} \sum(y_i-a)^2\]
\[a_{opt}=\overline{y}\]
А по нашему предположению $\overline{y}^*$ дает минимум меньше, чем мы получаем, оптимизируя.
Возникает противоречие.
\item
Величина $ESS$ может меняться как угодно.
Попробуем решить по аналогии с предыдущими пунктами:
\[\sum_{i=1}^n (\hat{y_i}^* - \overline{y}^*)^2 \leq \sum_{i=1}^{n+1} (\hat{y_i}^* - \overline{y}_i^*)^2 < \sum_{i=1}^{n} (\hat{y_i} - \overline{y})^2
\]
Видим, что провести аналогичные рассуждения мы не можем.
\begin{enumerate}
\item Величина $ESS$ может падать. Например,
Например, можно разместить точку настолько низко, чтобы прямая стала горизонтальной. Тогда все $\hat{y_i} = \overline{y}^*$ и $ESS=0$.
\item Величина $ESS$ может возрастать.
Расположим $\hat{y_i}$ на прямую, тогда величина $\hat{y}$ не меняется, величина $\overline{y}$ выросла, появилось новое слагаемое, из-за которого величина $ESS$ возрастает.
\end{enumerate}
\end{enumerate}

\section{Сингулярное разложение матрицы}
Английское сокращение: SVD — Singular Value Decomposition.
Сингулярное разложение матрицы используется при снижении размерности и не только.
\subsection{Смысл условия $U^T U= I$}
Вспомним смысл условия $U^T U= I$, где $I$ — единичная матрица.\\
\begin{enumerate}
\item  Порядок умножения не важен:
\[U^T U= I \Leftrightarrow U^{-1}=U^T \Leftrightarrow UU^T=I \]
\item  Рассмотрим вектор-столбцы матрицы $U$, $u_1, \ldots, u_n$:

\[ \begin{bmatrix}
\dots & u_1^T & \dots  \\
\dots & u_2^T & \dots  \\
\hdotsfor{3} \\
\dots & u_n^T & \dots
\end{bmatrix}
 \begin{bmatrix}
\vdots & \vdots &   & \vdots \\
u_1 & u_2 &  & u_n \\
\vdots & \vdots & & \vdots
\end{bmatrix} =
\begin{bmatrix}
1 & 0 & \hdots & 0 \\
0 & 1 & \hdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \hdots & 1 \\
\end{bmatrix}
\]
\[
u_2^T \cdot u_2 = 1
\]
\[
u_2^T \cdot u_7 = 0
\]
Следовательно, столбцы $u$ имеют единичную длину и ортогональны друг другу.\\
\end{enumerate}
\subsection{Свойства ортогональной матрицы}
\[ U \cdot x=\Tilde{x} \]
\begin{enumerate}
\item Матрица $U$ сохраняет длину вектора.\\
$\Tilde{x}^T \Tilde{x}= (Ux)^T (Ux)= x^T U^T U x = x^T I x=x^T x$
\item Матрица $U$ сохраняет угол.\\
$U \cdot x=\Tilde{x}$\\
$U \cdot y=\Tilde{y}$\\
$\cos(\Tilde{x},\Tilde{y})= \dfrac{\Tilde{x}^T\Tilde{y}}{\|\Tilde{x}\|\|\Tilde{y}\|}=
\dfrac{x^T U^T Uy}{\|x\|\|y\|}= \dfrac{x^Ty}{\|x\|\|y\|}=\cos(x,y)$\\
\end{enumerate}

Матрица $U$ может быть, например, поворотом или отражением.

%Переименовали
\subsection{Геометрия SVD}
Пусть $X$ — матрица всех переменных размера $n \times k$, где $k$ — количество переменных, $n$ — количество наблюдений.
Матрица $X$ преобразует $k$-мерные вектора в $n$-мерные.\\
\[X \cdot a = b\]

Любое действие $X$ при выборе удачного базиса в $\mathbb{R}^k$ и при выборе удачного базиса в $\mathbb{R}^n$ очень просто действует.

%опечатка
Допустим, в пространестве $R^k$ мы выбрали базис $v_1, v_2, \ldots, v_k$. А в $R^n$ мы выбрали базис $u_1, u_2, \ldots, u_n$.\\
Оказывается, можно так подобрать базисы, что:
\begin{equation*}
 \begin{cases}
   Xv_1=\lambda_1u_1
   \\
 Xv_2=\lambda_2u_2
 \\
 \ldots
 \end{cases}
\end{equation*}

%опечатка
Так как в общем случае $n \neq k$, то, либо $v$ не переходят в некоторые $u$, либо
если $k>n$, $Xv_j=0, j>n$.

\textbf{Пример}. Проецирование $X:\mathbb{R}^3 \rightarrow  \mathbb{R}^2$

С помощью сингулярного разложения независимо от действия можно выбрать хороший базис в $\mathbb{R} ^2$ и $\mathbb{R} ^3$, что действие будет очень просто. Выберем базис:
\begin{equation*}
 \begin{cases}
   Xv_1=u_1
   \\
 Xv_2=u_2
 \\
 Xv_3=0
 \end{cases}
\end{equation*}

\textbf{Пример}: Поворот на плоскости по часовой стрелке на $45$ градусов $X:\mathbb{R} ^2 \rightarrow  \mathbb{R} ^2$

Выберем базис:
\begin{equation*}
 \begin{cases}
   Xv_1=u_1
   \\
Xv_2=u_2
 \end{cases}
\end{equation*}
\subsection{Представление матрицы в виде произведения}
Любую матрицу $X$ размера $n \times k$ можно представить в виде $X = U \cdot \Sigma \cdot V^T,$ где:
$U^T\cdot U = I_{n \times n}$, $V^T\cdot V = I_{k \times k}$, т.е. действие $V$ не меняет углы и расстояния, а просто меняет базис.

Матрица $\Sigma$ диагональная, но необязательно квадратная. Например, \[\Sigma= \begin{pmatrix}
5 & 0 & 0  \\
0 & 6 & 0
\end{pmatrix}
\]
\[\Sigma= \begin{pmatrix}
1 & 0  \\
0 & 2  \\
0 & 0  \\
0 & 0
\end{pmatrix}
\]

Пусть $x_V$ — столбец координат в базисе $V$, а $x$ — столбец координат в исходном базисе, тогда
\[Vx_V=x
\]
\[x_V=V^T x
\]

%дополнение
Геометрический смысл разложения:
\begin{center}
$\xleftarrow
    [\text{привели в исходный базис}] 
    {U}
\xleftarrow
    [\text{подействовали в хороших координатах}]
    {\Sigma}
\xleftarrow
    [\text{привели в хорошие координаты}]
    {V^T}$
\end{center}


\subsection{Задача \href{https://github.com/bdemeshev/metrics_pro/raw/master/metrics_pro.pdf}{$5.14$}}
Дана матрица
$X= U\begin{pmatrix}
7 & 0 & 0 \\
0 & 3 & 0
\end{pmatrix} V^T$.

Найти сингулярное разложение матриц:

\begin{enumerate}
\item $X^T$
\item $10X$
\item $X^TX$
\item $XX^T$
\end{enumerate}

\textbf{Решение:}

\begin{enumerate}
\item $X^T = \Big( U\begin{pmatrix}
7 & 0 & 0 \\
0 & 3 & 0
\end{pmatrix} V^T\Big)^T=V\begin{pmatrix}
7 & 0 & 0 \\
0 & 3 & 0
\end{pmatrix}^T U^T =
V\begin{pmatrix}
7 & 0  \\
0 & 3  \\
0 & 0
\end{pmatrix} U^T$
\item Матрицы $U$, $V$ сохраняют длину, поэтому
$\Sigma = \begin{pmatrix}
70 & 0 & 0 \\
0 & 30 & 0
\end{pmatrix}$

В итоге:
\[10X= U\begin{pmatrix}
70 & 0 & 0 \\
0 & 30 & 0
\end{pmatrix} V^T
\]

\item
\[X^TX= V \Sigma^T U^T U \Sigma V^T= V \Sigma^T \Sigma V^T
\]
\[X^TX  = V\begin{pmatrix}
7 & 0  \\
0 & 3  \\
0 & 0
\end{pmatrix} \begin{pmatrix}
70 & 0 & 0 \\
0 & 30 & 0
\end{pmatrix} V^T=
V\begin{pmatrix}
49 & 0  \\
0 & 9
\end{pmatrix}  V^T
\]


\item \[XX^T= U \Sigma V^T V \Sigma^T U^T = U \Sigma \Sigma^T U^T
\]

\[ X^TX =U\begin{pmatrix}
70 & 0 & 0 \\
0 & 30 & 0
\end{pmatrix} \begin{pmatrix}
7 & 0  \\
0 & 3  \\
0 & 0
\end{pmatrix} U^T =
U\begin{pmatrix}
49 & 0 & 0 \\
0 & 9 & 0 \\
0 & 0 & 0
\end{pmatrix} U^T
\]
\end{enumerate}

\subsection{Алгоритм SVD разложения компьютером}

\begin{enumerate}
\item Пусть $X=UBV^T$, где $U$, $V$ — отражение, а $B$ — бидиагональная матрица.
\[
B = \begin{pmatrix}
a_{1,1} & a_{1,2} & 0 & \hdots & 0 \\
0 & a_{1,1} & a_{1,2} & \hdots & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \hdots & a_{n-1,n-1} & 0 \\
0 & \hdots & 0 & a_{n,n-1} & a_{n,n} &
\end{pmatrix}
\]
На первом этапе действия алгоритма побираются такие отражения, чтобы получить бидиагональную матрицу. Этот этап условно точный, мы можем его выписать численно.
\item Этот этап численный. С помощью вращений  $X = U \Sigma V^T$ достигаем конечного результата с какой-то точностью.
\end{enumerate}

Важно, что на компьютере при сингулярном разложении матрицы $X$ никогда не считается $X^TX$.
\subsection{SVD-разложение руками}
\textbf{Задача.} Найти SVD-разложение матрицы

\[
\begin{pmatrix}
1 & 1  \\
0 & 1 \\
-1 & 0
\end{pmatrix}
\]

\textbf{Решение.}
\[
X = \begin{pmatrix}
1 & 1  \\
0 & 1 \\
-1 & 0
\end{pmatrix} = U \Sigma V^T
\]
\[X^TX = V \Sigma^T \Sigma V^T= = \begin{pmatrix}
2 & 1 \\
1 & 2
\end{pmatrix}\]
\[XX^T= U \Sigma \Sigma^T U^T= \begin{pmatrix}
2 & 1 & -1 \\
1& 1 & 0 \\
-1 & 0 & 1 \\
\end{pmatrix}\]

Найдем собственные числа матрицы $X^TX$:

\[
\det \begin{pmatrix}
2-\lambda & 1 \\
1 & 2-\lambda
\end{pmatrix}=0 \rightarrow \lambda = 3,1
\]

Собственные вектора матрицы $X^TX$ равны $\begin{pmatrix}
1 \\
1
\end{pmatrix}$, $\begin{pmatrix}
1 \\
-1
\end{pmatrix}$

Нормированная матрица:
$V = \begin{pmatrix}
\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} \\
\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
\end{pmatrix} $

Тогда
\[
X^TX = \begin{pmatrix}
\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} \\
\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
\end{pmatrix}
\begin{pmatrix}
3 & 0 \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} \\
\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
\end{pmatrix}^T
\]

Собственные числа для матрицы $XX^T$:
\[
\det \begin{pmatrix}
2-\lambda & 1 & -1 \\
1& 1-\lambda & 0 \\
-1 & 0 & 1-\lambda \\
\end{pmatrix}=0 \rightarrow \lambda = 0,1,3
\]

Собственные вектора матрицы $XX^T$: $\begin{pmatrix}
1 \\
-1 \\
1
\end{pmatrix}$, $\begin{pmatrix}
0 \\
1\\
1\\
\end{pmatrix}$,
$\begin{pmatrix}
2 \\
1\\
-1\\
\end{pmatrix}$

Нормированная матрица $U$:
\[
\begin{pmatrix}
\dfrac{1}{\sqrt{3}} & 0 & \dfrac{2}{\sqrt{6}} \\
-\dfrac{1}{\sqrt{3}} & \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{6}} \\
\dfrac{1}{\sqrt{3}} & \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{6}}
\end{pmatrix}
\]

\[
\Sigma = \begin{pmatrix}
\sqrt{3} & 0 \\
0 & 1 \\
0 & 0 \\
\end{pmatrix}
\]

В итоге получаем разложение исходной матрицы $X$:

\[
X= \begin{pmatrix}
\dfrac{1}{\sqrt{3}} & 0 & \dfrac{2}{\sqrt{6}} \\
-\dfrac{1}{\sqrt{3}} & \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{6}} \\
\dfrac{1}{\sqrt{3}} & \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{6}}
\end{pmatrix}  \begin{pmatrix}
\sqrt{3} & 0 \\
0 & 1 \\
0 & 0 \\
\end{pmatrix}
\begin{pmatrix}
\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} \\
\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
\end{pmatrix}
\]

\section{Метод главных компонент (Principal Component Analysis)}
\subsection{Задача}
Сформулируем задачу. Она состоит в том, чтобы снизить размерность матрицы $Х$ таким образом, чтобы несильно двигать точки.
Идея: пусть у нас есть две координаты. Тогда мы возьмем двумерное облако точек. Мы хотим несильно двигая точки сделать так, чтобы точки лежали на одной прямой.

%добавим рисунок

\begin{center}
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt       
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300

%Shape: Axis 2D [id:dp24759014475003494] 
\draw  (50,253.3) -- (286.5,253.3)(73.65,58) -- (73.65,275) (279.5,248.3) -- (286.5,253.3) -- (279.5,258.3) (68.65,65) -- (73.65,58) -- (78.65,65)  ;
%Straight Lines [id:da3232722846352455] 
\draw    (47.5,281) -- (88.5,238) -- (117.5,209) -- (148.5,179) -- (178.5,150) -- (207.5,123) -- (232.5,99) -- (275.5,57) ;


%Straight Lines [id:da8753321180769544] 
\draw    (95.5,187) -- (117.5,209) ;

\draw [shift={(95.5,187)}, rotate = 45] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
%Straight Lines [id:da19651042514081052] 
\draw    (232.5,99) -- (265.5,133) ;
\draw [shift={(265.5,133)}, rotate = 45.86] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;

%Straight Lines [id:da9109072984148011] 
\draw    (233.5,57) -- (254,78) ;

\draw [shift={(233.5,57)}, rotate = 45.69] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
%Straight Lines [id:da3432048448041404] 
\draw    (207.5,123) -- (260.5,178) ;
\draw [shift={(260.5,178)}, rotate = 46.06] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;

%Straight Lines [id:da39076839836091726] 
\draw    (166.5,110) -- (193,136.5) ;

\draw [shift={(166.5,110)}, rotate = 45] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;
%Straight Lines [id:da5973700502490836] 
\draw    (178.5,150) -- (244.5,219) ;
\draw [shift={(244.5,219)}, rotate = 46.27] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;

%Straight Lines [id:da8167246089592133] 
\draw    (87.5,113) -- (149,174) ;

\draw [shift={(87.5,113)}, rotate = 44.77] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]      (0, 0) circle [x radius= 3.35, y radius= 3.35]   ;




\end{tikzpicture}
\end{center}



Пусть $x_i$ — транспонированная $i$-ая строка в матрице $X$ или столбец в матрице $X^T$.
Тогда наша задача сводится к задаче оптимизации:
\[\min_{\mu, V, \lambda_i} \sum ||x_i-\mu-V \lambda_i||^2 \]
Вектор $x_i$ имеет размер $k \times 1$, вектор $\mu$ имеет размер $k \times 1$, вектор $\lambda_i$ имеет размер $p \times 1$, матрица $V$ имеет размер $k \times p$.

В столбцах матрицы $V$ находятся $k-$мерные ортогональные вектора, единичной длины. Вектор $\lambda_i$ показывает веса, с которыми надо взять эти вектора, пытаясь заменить каждый $x_i$. Вектор $\mu$ является серединой облака. Число $k$ — исходная размерность, число $p$ — размерность, до которой хотим снизить. Для визуализации обычно берут $p=2$.

Предположим, что матрица $V$ уже найдена. Оказывается, неважно, до какой размерности мы снижали, $\mu=\dfrac{\sum x_i}{n}$, а $\lambda_i$ находится легко.

Допустим вектор $\mu$ и матрица $V$ найдены. Оптимальный вектор $\lambda_1$ находим, решая задачу оптимизации:
\[\min_{\lambda_1} ||x_1-\mu-V {\lambda_1}||^2 \]

Это есть аналог МНК \[ \beta:  \min_\beta ||y-X\beta||^2 \]

\[\hat{\beta}=(X^TX)^{-1}X^Ty \]

В нашем случае:

\[\lambda_i=(V^TV)^{-1}V^T(x_i-\mu)=V^T(x_i-\mu)\]

На самом деле метод главных компонент — это выбор плоскости, а дальше МНК.

\begin{center}
\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,427); %set diagram left start at 0, and has height of 427

%Straight Lines [id:da07315977373396598] 
\draw    (146.55,324.86) -- (340.03,145.36) ;
\draw [shift={(341.5,144)}, rotate = 497.15] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

%Straight Lines [id:da6804977884743226] 
\draw [color={rgb, 255:red, 23; green, 18; blue, 206 }  ,draw opacity=1 ]   (146.55,324.86) -- (222.87,301.7) -- (231,299.33) -- (337.19,267.81) ;
\draw [shift={(339.11,267.24)}, rotate = 523.47] [color={rgb, 255:red, 23; green, 18; blue, 206 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

%Curve Lines [id:da3086901590160096] 
\draw    (205.5,195) .. controls (237.5,185) and (464.74,159.56) .. (450.65,196.89) .. controls (436.56,234.22) and (485.87,214.12) .. (503.48,225.6) .. controls (521.1,237.09) and (505.83,258.62) .. (474.13,270.11) .. controls (442.43,281.6) and (514.05,281.6) .. (497.61,310.31) .. controls (481.18,339.03) and (417.77,339.03) .. (387.25,339.03) ;


%Straight Lines [id:da7422567753451483] 
\draw [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ] [dash pattern={on 0.84pt off 2.51pt}]  (341.5,144) -- (339.11,267.24) ;


%Straight Lines [id:da7019841586401416] 
\draw  [dash pattern={on 0.84pt off 2.51pt}]  (278.5,101) -- (267.5,238) ;


%Straight Lines [id:da6469168723121977] 
\draw [color={rgb, 255:red, 16; green, 17; blue, 209 }  ,draw opacity=1 ]   (280.5,322.04) -- (148.22,324.57) ;

\draw [shift={(282.5,322)}, rotate = 178.91] [color={rgb, 255:red, 16; green, 17; blue, 209 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Curve Lines [id:da3537592380937872] 
\draw    (387.25,339.03) .. controls (330.5,339) and (208.5,372) .. (168.5,354) .. controls (128.5,336) and (117.5,312) .. (126.5,276) .. controls (135.5,240) and (185.5,201) .. (205.5,195) ;


%Straight Lines [id:da7055118642625359] 
\draw    (148.22,324.57) -- (277.49,102.73) ;
\draw [shift={(278.5,101)}, rotate = 480.23] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

%Straight Lines [id:da5999353284374774] 
\draw [color={rgb, 255:red, 32; green, 11; blue, 224 }  ,draw opacity=1 ]   (148.22,324.57) -- (265.88,239.17) ;
\draw [shift={(267.5,238)}, rotate = 504.03] [color={rgb, 255:red, 32; green, 11; blue, 224 }  ,draw opacity=1 ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

%Straight Lines [id:da5443929107503771] 
\draw    (148.22,324.57) -- (280.75,251.96) ;
\draw [shift={(282.5,251)}, rotate = 511.28] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

%Straight Lines [id:da21630118538032628] 
\draw  [dash pattern={on 0.84pt off 2.51pt}]  (282.5,251) -- (282.5,322) ;



% Text Node
\draw (354.61,136.18) node [scale=1]  {$x_{2} \ $};
% Text Node
\draw (289.25,335.05) node [color={rgb, 255:red, 26; green, 19; blue, 211 }  ,opacity=1 ]  {$\hat{x}_{3}$};
% Text Node
\draw (344.35,282.01) node [color={rgb, 255:red, 17; green, 10; blue, 184 }  ,opacity=1 ]  {$\hat{x}_{2}$};
% Text Node
\draw (292.61,96.18) node [scale=1]  {$x_{1} \ $};
% Text Node
\draw (294.61,246.18) node [scale=1]  {$x_{3} \ $};
% Text Node
\draw (282.35,228.01) node [color={rgb, 255:red, 24; green, 18; blue, 187 }  ,opacity=1 ]  {$\hat{x}_{1}$};
% Text Node
\draw (359,87) node   {$\mathbb{R}^{k}$};
% Text Node
\draw (434,241) node   {$v_{1} ...v_{p}$};


\end{tikzpicture}
\end{center}


\par
\textbf{Почему $V^TV = I$?}

Пусть $x_1, x_2, x_3, \ldots$ — вектора наблюдений: $x_i$ — $i$-ое наблюдение, в отличие от МНК, где $x_1, x_2, x_3, \ldots$ — регрессоры. Ищем подпространство, чтобы при проецировании на него получались $\hat{x}_1, \hat{x}_2, \ldots$. Это подпространство будем задавать базисом $v_1, \ldots, v_p$. Для удобства задаем подпространство ортонормальным базисом.
\[
\begin{bmatrix}
v_1^T \\
v_2^T
\end{bmatrix}
\begin{bmatrix}
v_1 & v_2
\end{bmatrix}=
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\]

Вектор $\lambda_i$ мы уже нашли с помощью МНК, теперь осталось проминизировать по $\mu, V$:

\[\min_{\mu, V}||(x_i-\mu)-VV^T(x_i-\mu)||^2=\min_{\mu, V} Q_\mu\]

Матрица $VV^T$ — проектор, матрица-шляпница.

Выпишем условие первого порядка:
\[dQ_\mu = \sum_i ||(I-VV^T)(x_i-\mu)||^2 = \sum_i d((x_i - \mu)^T(I-VV^T)^T(I-VV^T)(x_i - \mu)) = \sum_i d (x_i - \mu)^T (I-VV^T)(x_i - \mu)= \]
\[= \sum_i -2 (x_i - \mu)^T (I-VV^T)d\mu = 0\]

В итоге решение $\hat{\mu} = \dfrac{\sum x_i}{n}$ зануляет дифференциал $dQ$. Но оно нединственно, можно придумать и другие.
\end{document}