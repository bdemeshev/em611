\documentclass[12pt]{article} % размер шрифта

\usepackage{tikz} % картинки в tikz
\usepackage{microtype} % свешивание пунктуации

\usepackage{array} % для столбцов фиксированной ширины

\usepackage{url} % для вставки ссылок \url{...}

\usepackage{indentfirst} % отступ в первом параграфе

\usepackage{sectsty} % для центрирования названий частей
\allsectionsfont{\centering} % приказываем центрировать все sections

\usepackage{amsthm} % теоремы и доказательства

\theoremstyle{definition} % прямой шрифт в условии теорем
\newtheorem{theorem}{Теорема}[section]


\usepackage{amsmath, amssymb} % куча стандартных математических плюшек

\usepackage[top=2cm, left=1.5cm, right=1.5cm, bottom=2cm]{geometry} % размер текста на странице

\usepackage{lastpage} % чтобы узнать номер последней страницы

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке
\usepackage{caption} % подписи к картинкам без плавающего окружения figure

\usepackage{hyperref} % гиперссылки

\usepackage{verbatim} % побуквенный вывод

\usepackage{fancyhdr} % весёлые колонтитулы
\pagestyle{fancy}
\lhead{Эконометрика, финтех}
\chead{}
\rhead{2018-12-1, встреча 8}
\lfoot{}
\cfoot{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}



\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет картина Последний день Помпеи}
% команда \listoftodos — печатает все поставленные \todo'шки

\usepackage{booktabs} % красивые таблицы
% заповеди из документации:
% 1. Не используйте вертикальные линии
% 2. Не используйте двойные линии
% 3. Единицы измерения помещайте в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"

\usepackage{fontspec} % поддержка разных шрифтов
\usepackage{polyglossia} % поддержка разных языков

\setmainlanguage{russian}
\setotherlanguages{english}

\setmainfont{Linux Libertine O} % выбираем шрифт
% если Linux Libertine не установлен, то
% можно также попробовать Helvetica, Arial, Cambria и т.Д.

% чтобы использовать шрифт \Linux Libertine на личном компе,
% его надо предварительно скачать по ссылке
% http://www.\Linuxlibertine.org/index.php?id=91&L=1

% на сервисах типа sharelatex.com этот шрифт есть :)

\newfontfamily{\cyrillicfonttt}{Linux Libertine O}
% пояснение зачем нужно шаманство с \newfontfamily
% http://tex.stackexchange.com/questions/91507/

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
\setlist[enumerate, 2]{label=\asbuk*),ref=\asbuk*} % списки уровня 2 будут буквами а) б) ...

%% эконометрические и вероятностные сокращения
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sCov}{sCov}
\DeclareMathOperator{\sVar}{sVar}
\DeclareMathOperator{\sCorr}{sCorr}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\tr}{trace}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\Lin}{Lin}
\DeclareMathOperator{\Linp}{Lin^{\perp}}


\def \hb{\hat{\beta}}
\def \hs{\hat{\sigma}}
\def \htheta{\hat{\theta}}
\def \s{\sigma}
\def \hy{\hat{y}}
\def \hY{\hat{Y}}
\def \v1{\vec{1}}
\def \e{\varepsilon}
\def \he{\hat{\e}}
\def \z{z}
\def \hVar{\widehat{\Var}}
\def \hCorr{\widehat{\Corr}}
\def \hCov{\widehat{\Cov}}
\def \cN{\mathcal{N}}
\def \RR{\mathbb{R}}

\makeatletter
\def\MT@warn@unknown{}
\makeatother

\begin{document}
Конспектировали: Марк Любимов Галим Бикбов.\\

\textbf{Упражнение}:\\
Пусть даны факторы $x$, $y$, $z$, тогда обозначим матрицу, состоящую из данных факторов, как:
\[
\ X = \begin{pmatrix}
           \ x_{1} & \ y_{1} &\ z_{1}\\
           \ x_{2} & \ y_{2} &\ z_{2}\\
           \vdots & \vdots &  \vdots\\
           \ x_{n} & \ y_{n} &\ z_{n}\\
         \end{pmatrix}
         \]\\
$\widetilde{X}$ - центрированная матрица для X.\\
Дана ковариационная матрица:
\[\frac{\widetilde{X}^T\cdot\widetilde{X}}{(n-1)} = \begin{pmatrix}
            \ 28 & 110 & 844\\
            \ 110 & 644 & 3429 \\ 
            \ 844 & 3429 & 26585\\
        \end{pmatrix}
\]

\[\bar{x} = 15.4;\]
\[\bar{y} = 43;\]
\[\bar{z} = 265;\]
\[n = 50.\]\\
Рассматривается регрессионная модель:\\
\[\hat{y}_i=\hat{\beta}_1+\hat{\beta}_xx_i+\hat{\beta}_zz_i\]
Найти:\\
\begin{enumerate}
    \item $\hat{\beta}_x, \hat{\beta}_x, \hat{\beta}_1$;
    \item RSS, TSS, ESS;
    \item При условии, что наблюдений много и $\alpha = 0.05$, проверить гипотезу $H_0$ против $H_1$:  
    \begin{enumerate}
        \item $H_0: \beta_x = 0$
        \item $H_1: \beta_x \neq 0$
    \end{enumerate}
\end{enumerate}
\textit{Решение}:\\
Зная ковариационную матрицу, можно найти $W$ и $M$
\[W = \widetilde{X}^T\cdot\widetilde{X} = \begin{pmatrix}
            \ 1372 & 5390 & 41356\\
            \ 5390 & 31556 & 168021 \\ 
            \ 41356 & 168021 & 1302665\\
        \end{pmatrix}
\]
\[M = W^{-1} = \begin{pmatrix}
            \ 0.01702223 & -0.0000961 & -0.00052801\\
            \ -0.0000961 &  0.00010171 & -0.00001007 \\
            \ -0.00052801 & -0.00001007 &  0.00001883\\
        \end{pmatrix}
\]
Вспомним,что предскавляют собой элементы матрицы $M$
 \[ 
 M = W^{-1} = \\
    \begin{pmatrix}
         \ldots & \ldots & \ldots \\
        \cfrac{-\hb_x}{RSS} & \cfrac{1}{RSS} & \cfrac{-\hb_z}{RSS}  \\
        \ldots & \ldots & \ldots \\
     \end{pmatrix}
 \]
Тогда: 
\[RSS = \dfrac{1}{0.00010171} =  9831.87\]    
следовательно
\[\hb_x = 0.0000961\cdot RSS = 0.945\]
и
\[\hb_z = 0.00001007\cdot RSS = 0.099\]
Чтобы найти $\hat{\beta}_1$, просуммируем все $\hat{y}_i$ и поделим на $n$
\[\bar{y}_i=\hat{\beta}_1+\hat{\beta}_x\bar{x}_i+\hat{\beta}_z\bar{z}_i\]
Отсюда найдем $\hat{\beta}_1$:
\[\hat{\beta}_1 = \bar{y} - \hat{\beta}_x\bar{x} - \hat{\beta}_z\bar{z} = 43 - 0.945\cdot15.4 - 0.099\cdot265 = 2.21\]    
TSS известно из матрицы $W$:
\[TSS = 31556\]
Теперь легко посчитать $R_2$:
\[R^2 = 1 - \dfrac{RSS}{TSS} = 0.688\]
Перейдем к проверке гипотезы. Для этого необходимо посчитать статистику $t$:
\[t = \dfrac{\hat{\beta}_x - 0}{se(\hat{\beta}_x)}\]
Дисперсии регрессионных коэффициентов равны:
\[\Var \dbinom{\hat{\beta}_x}{\hat{\beta}_z} = \dfrac{RSS}{n-k}\cdot{(\widetilde{X}^{T}_y \cdot \widetilde{X}_y)^{-1}}
\]
где \[
 X_y = \begin{pmatrix}
           \ x_{1} &\ z_{1}\\
           \ x_{2} &\ z_{2}\\
           \vdots & \vdots\\
           \ x_{n} &\ z_{n}\\
         \end{pmatrix}
         \]
         
\[(\widetilde{X}^{T}_y \cdot \widetilde{X}_y)^{-1} = \begin{pmatrix}
            \ 451.2 & 12651.8\\
            \ 12651.8 & 407874.1\\
        \end{pmatrix}
\]
Тогда стандартная ошибка равна:
\[se(\hat{\beta}_x) = \sqrt{\dfrac{451.2\cdot9831.87}{47}} = 307.2\]
Посчитаем статистику $t$:
\[t = \dfrac{0.945}{307.2} = 0.003 < 1.68\]
Таким обращом гипотеза не отвергается. Решение для $\hat{\beta}_z$ аналогично.\\

\textbf{Утверждение}: Выразить \[\Var(\hat{\beta}_x^{множественная})\] 
Пусть даная центрированная матрица $X$. Рассмотрим регрессию
\[\hat{y} = X\cdot{\hat\beta}\]
Выведем формулу для частного случая: 7-го коэффциента.
\[ \Var(\hat{\beta}_7) = \sigma^2\cdot(X^{T}_y \cdot X_y)^{-1} = \dfrac{\sigma^2}{RSS_7} = \dfrac{\sigma^2}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}\cdot\dfrac{1}{\frac{RSS_7}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}} = \dfrac{\sigma^2}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}\cdot\dfrac{1}{1-R^2_7}\]
то есть
\[\Var(\hat{\beta}_x^{mult}) =  \Var(\hat{\beta}_x^{simple})\cdot\dfrac{1}{1 - R_x^2} = \Var(\hat{\beta}_x^{simple})\cdot\dfrac{1}{VIF_x}\]

\textbf{Головоломка}:
\par Злобный Дракон поймал двух принцесс, Читу и Ниту, и посадил
их в разные башни своего замка. Затем Злобный Дракон подбросил
правильную монетку бесконечное число раз. Все результаты
чётных бросков он сообщил Чите, а все результаты нечётных —
Ните. Далее Дракон предлагает каждой из принцесс назвать номер
любого подбрасывания, результат которого ей не известен. То есть
Чита должна назвать нечётный номер, а Нита — чётный.
\par
Если результаты бросков, названных Читой и Нитой, одинаковые,
то Злобный Дракон дарит каждой принцессе тортик, розового
плюшевого зайца и отпускает на свободу. Если же результаты
бросков отличаются, то Злобный Дракон съедает Читу и Ниту с
клюквенным вареньем. Дракон обожает принцесс и клюквенное
варенье!
\par Принцессы знают о повадках Злобного Дракона и могли заранее до похищения договориться о стратегиях.
\begin{enumerate}
\item{Каковы шансы принцесс спастись, если Нита называет число
первой, а Чита называет своё число, зная что выбрала Нита?}
\item{Каковы шансы принцесс спастись, если Чита и Нита после
похищения лишены возможности передавать информацию
друг другу?}
\end{enumerate}
\par\textbf{Решение находится после Гетероскедастичности}

\section{Гетероскедатичность}

Среди условий теоремы Гаусса-Маркова есть предпосылка $\Var(u_i(x)) = \sigma ^2 $ . Теперь  $\Var(u_i | x)) = h(x_i)$ 

Теорма Гаусса-Маркова неприменима, тогда что случится с оценкой $\hat{\beta}  = (X^T X)^{-1} X^{-1} Y$   ? 
  
\textbf{Сохраняется:}

\begin{enumerate}
\item единственность
\item линейность по $y$
\item несмещенность $E(u_i |x) = 0$
\end{enumerate}

\par

Состоятельность сохраняется при некоторых дополнительных предпосылках.

\par

\textbf{Пропало:}
\begin{enumerate}
\item У $ \hat{\beta}  $  наименьшая дисперсия среди других линейно несмещенных оценок
\item  теорема о распределении разных статистик
\end{enumerate}
\[ \frac{ \hat{\beta} - \beta }{se(\hat\beta)} \sim  t_{n-k}  \]
\[ \frac{(RSS_R - RSS_{UR})/(k_{UR} - k_R)}{RSS_{UR}/(n-k_{UR})} \sim F_{k_{UR} - k_R
, n-k_{UR}}   \]

где $k_R$ ~---  число регрессоров в регрессии с ограничениями, $k_{UR}$ ~--- число регрессоров в регрессии без ограничений
\par

\par
Сами оценки ничего, они не плохие, но нельзя проверять гипотезы.
\par


\textbf{Упр.}
 Хотим скорректировать формулы:
\begin{enumerate}

\item
\[y_i = \beta x_i + u_i\]
\[\Var(u_i |x_i) = {\sigma_i}^2 \]
\[\Var(\hat{\beta} |x) = \Var \left (\frac{\displaystyle\sum\ {x_i y_i}}{\displaystyle\sum\ {x_i} ^2}|x \right) =\frac{\displaystyle\sum\ {x_i}^2 \Var(y_i|x_i)}{\displaystyle\sum\ {x_i} ^2} = \frac{\displaystyle\sum\ {x_i}^2 {\sigma_i}^2}{{(\displaystyle\sum\ {x_i}) ^2}}  \]
\item
\[y = X \cdot \beta + u\]
\[\Var(u|x) = \Omega  \]
\[\Var(\hat{\beta} |x) =  \Var ((X^T X)^{-1} X^{-1} y |X ) = X^T X^{-1} X^T \Var(y|X) X (X^T X)^{-1} =\] 
\[ =  (X^T X) ^{-1} X^T \Omega X (X^T X)^ {-1}\]

раньше $ \Omega_{ii} = {\sigma_{i}} ^2$,  
$\Var (\hat{\beta} |x) = \frac {RSS}{n-k} (X^T X)^{-1}$
\end{enumerate}
\par
Как оценить ${\sigma_i}^2$ ? 
\[{\sigma_i}^2 = \Var(u_i |X) = E({u_i}^2 |X) \]
\par
\textbf{Корректировка Уайта, $HC_0$}
\[E({u_i}^2|x) \approx {u_i}^2 \approx {\hat{u_i}^2}\]

\[{\hat{\Var}}_{HC_0} (\hat{\beta}|X) = (X^T X)^{-1} X^T \hat{\Omega} X (X^T X) ^ {-1}   \]
\textbf{Утверждение. Если :}
\begin{enumerate}

\item  $y = X \cdot \beta + u$
\item  $\Var(u|X) = \Omega $
\item  $\beta$ ~---  константа, $ {{\sigma}_i}^2 = f(x_i)$ 
\item  Строки $x_i$ независимы и одинаково распределены
\item  $E({x^4}_{ij}) < \infty$
\end{enumerate}

\textbf{То :}
\[ \frac{ \hat{\beta} - \beta }{se_{HC}(\hat{\beta})}  \:\xrightarrow[n\longrightarrow \infty]\: N(0;1)\]

\textbf{Упр. $HC_1$}

\[ \Omega E(  {u_i}^2     | X        )  = n \cdot {\sigma}^2\]
\[ \Omega E(  {{\hat{u}}_i}^2     | X        )  = E(RSS|X) = (n-k) \cdot  {\sigma}^2\]

 \[{\hat{\Omega}}_{HC_1} = \frac{n} {n-k} {\hat{\Omega}}_{HC_0}  \]


\textbf{Упр. $HC_2$} 
\[ \Omega E(  {u_i}^2     | X        )  = n \cdot {\sigma}^2\]
\[ \Omega E(  {{\hat{u}}_i}^2     | X        )  =  \Var( {\hat{u}_i}|X) \]
\[\hat{u} = (I-H)y = y - \hat{y} = y-Hy\]
\[\Var(u_i |X) = (1 - h_{ii}) \cdot {\sigma}^2\]


Корректировка $HC_3$ основана на кроссвалидации.


\textbf{Экспериментальное исследование: }
\par
$HC_0$ плохо
\par
$HC_1$ лучше $HC_0$
\par
$HC_2$ лучше $HC_1$
\par
$HC_3$ немного лучше $HC_2$
\par

\addcontentsline{toc}{chapter}{Литература}
\begin{thebibliography}{99}

\bibitem{catheydowskinewparadigm} 
\href{https://cran.r-project.org/web/packages/sandwich/vignettes/sandwich.pdf} {\textcolor{blue}{Achim Zeileis,
Econometric Computing with HC and HAC 
Covariance Matrix Estimators}}

\end{thebibliography}
\par\textbf{Возможное решение задачи про Дракона}:
\begin{enumerate} 
\item{Нита называет такой номер, до которого был орёл, а после —
Решка. Чита, зная, что находится на названном Нитой месте,
однозначно угадывает.}
\item{Приведём пример стратегии, гарантирующей вероятность спасения 2/3. Чита отступает от первого орла на бросок назад, а Нита — на бросок вперёд.}
\end{enumerate}
\end{document}