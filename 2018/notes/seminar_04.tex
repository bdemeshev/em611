\documentclass[12pt]{article} % размер шрифта

\usepackage{tikz} % картинки в tikz
\usepackage{microtype} % свешивание пунктуации

\usepackage{array} % для столбцов фиксированной ширины

\usepackage{url} % для вставки ссылок \url{...}

\usepackage{indentfirst} % отступ в первом параграфе

\usepackage{sectsty} % для центрирования названий частей
\allsectionsfont{\centering} % приказываем центрировать все sections

\usepackage{amsthm} % теоремы и доказательства

\theoremstyle{definition} % прямой шрифт в условии теорем
\newtheorem{theorem}{Теорема}[section]


\usepackage{amsmath, amssymb} % куча стандартных математических плюшек

\usepackage[top=2cm, left=1.5cm, right=1.5cm, bottom=2cm]{geometry} % размер текста на странице

\usepackage{lastpage} % чтобы узнать номер последней страницы

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке
\usepackage{caption} % подписи к картинкам без плавающего окружения figure

\usepackage{hyperref} % гиперссылки

\usepackage{verbatim} % побуквенный вывод

\usepackage{fancyhdr} % весёлые колонтитулы
\pagestyle{fancy}
\lhead{Эконометрика, финтех}
\chead{}
\rhead{2018-10-13, встреча 4}
\lfoot{}
\cfoot{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}



\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет картина Последний день Помпеи}
% команда \listoftodos — печатает все поставленные \todo'шки

\usepackage{booktabs} % красивые таблицы
% заповеди из документации:
% 1. Не используйте вертикальные линии
% 2. Не используйте двойные линии
% 3. Единицы измерения помещайте в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"

\usepackage{fontspec} % поддержка разных шрифтов
\usepackage{polyglossia} % поддержка разных языков

\setmainlanguage{russian}
\setotherlanguages{english}

\setmainfont{Linux Libertine O} % выбираем шрифт
% если Linux Libertine не установлен, то
% можно также попробовать Helvetica, Arial, Cambria и т.Д.

% чтобы использовать шрифт \Linux Libertine на личном компе,
% его надо предварительно скачать по ссылке
% http://www.\Linuxlibertine.org/index.php?id=91&L=1

% на сервисах типа sharelatex.com этот шрифт есть :)

\newfontfamily{\cyrillicfonttt}{Linux Libertine O}
% пояснение зачем нужно шаманство с \newfontfamily
% http://tex.stackexchange.com/questions/91507/

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
\setlist[enumerate, 2]{label=\asbuk*),ref=\asbuk*} % списки уровня 2 будут буквами а) б) ...

%% эконометрические и вероятностные сокращения
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sCov}{sCov}
\DeclareMathOperator{\sVar}{sVar}
\DeclareMathOperator{\sCorr}{sCorr}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\tr}{trace}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\Lin}{Lin}
\DeclareMathOperator{\Linp}{Lin^{\perp}}


\def \hb{\hat{\beta}}
\def \hs{\hat{\sigma}}
\def \htheta{\hat{\theta}}
\def \s{\sigma}
\def \hy{\hat{y}}
\def \hY{\hat{Y}}
\def \v1{\vec{1}}
\def \e{\varepsilon}
\def \he{\hat{\e}}
\def \z{z}
\def \hVar{\widehat{\Var}}
\def \hCorr{\widehat{\Corr}}
\def \hCov{\widehat{\Cov}}
\def \cN{\mathcal{N}}
\def \RR{\mathbb{R}}

\makeatletter
\def\MT@warn@unknown{}
\makeatother




\begin{document}

Конспектировал: Ефим Лубошников.

\section{Свойства математического ожидания}
Вспомним, какими свойствами обладает математическое ожидание случайной величины и случайного вектора.
Обозначим:
$y_1$, $y_2$ — случайные величины,
$y$, $z$ — случайные вектора,
$a$ — скалярная константа,
$A$, $B$ — матрицы констант.

\begin{center}
\begin{tabular}{ll}
\toprule
Случайная величина                            & Случайный вектор                                    \\
\midrule
$\E(ay_1)=a\E(y_1)$                           & $\E(AyB)=A\E(y)B$                                   \\
$\Var(y_1)=\E(y_1^2)-\E(y_1)^2$               & $\Var(y)=\E(yy^T)-\E(y)\E(y^T)$                     \\
$\Var(ay_1)=a^2\Var(ay_1)$                    & $\Var(Ay)=A(\E(yy^T)-\E(y)\E(y^T))A^T$              \\
$\Var(a+y_1)=\Var(y_1)$                       & $\Var(a+y)=\Var(y)$                                 \\
$\Cov(y_1,y_2)=\E(y_1y_2)-\E(y_1)\E(y_2)$     & $\Cov(y,z)=\E(yz^T)-\E(y)\E(z^T)$                   \\
                                              & $\Cov(Ay,Bz)=A\Cov(y,z)B^T$                         \\
                                              & $\Cov(y,z)=\Cov(z,y)^T$                             \\
                                              & Если $ y,z \in \mathbb {R}^n $, то справедливо:     \\
                                              & $\Cov(y+z,w)=\Cov(y,w)+\Cov(z,w)$                   \\
                                              & $\Var(y+z)=\Var(y)+\Var(z)+\Cov(y,z)+\Cov(z,y)$     \\
\bottomrule
\end{tabular}
\end{center}





Так выглядит дисперсия случайного вектора $y \in \mathbb {R}^n $:
\[
\Var(y) = \begin{bmatrix}
           \Var(y_{1}) & \Cov(y_{1},y_{2}) & \ldots &\Cov(y_{1},y_{n})\\
           \Cov(y_{2},y_{1}) & \Var(y_{2}) & \ldots &\Cov(y_{2},y_{n})\\
           \vdots & \vdots & \ldots & \vdots\\
           \Cov(y_{n},y_{1}) & \Cov(y_{n},y_{2}) & \ldots &\Var(y_{n})\\
         \end{bmatrix}
         \]


\section{Предпосылки для теоремы Гаусса-Маркова}
Рассмотрим задачу регрессии $y=X\beta+u$ в случае двух регрессоров: $y_i=\beta_1+\beta_2x_i+\beta_3z_i+u_i$
Предпосылки:
\begin{enumerate}
    \item Предполагаем $\beta$ — неизвестная константа.
    \item $X$ должен иметь полный ранг, чтобы оценки МНК существовали. $X$ может быть:
    \begin{enumerate}
        \item известная константа, $y_t=\beta_1+\beta_2t+u_t$
        \item наблюдение - случайная величина, временной ряд.
    \end{enumerate}
    \item Гомоскедастичность: $\Var(u_i|X)=\sigma^2I$
    \item Отсутствие автокорреляции: $\Cov(u_i,u_j|x)=0, при i\neq j$
    \item$ \E(u|X)=0$
\end{enumerate}
\quad Обозначим:
\begin{itemize}
    \item $\hat{y}$  —  предсказанный $y$ на тренировочной выборке.
    \item $\hat{y}_{test}$  —  предсказанный $y$ на тестовой выборке.
\end{itemize}





\subsection{Упражнение "Мега-матрица"}
\textit{Задание}: Заполнить матрицу $\Var$, $\Cov$ для соответсвующих элементов матрицы:
\\\\
\begin{tabular}{c||rrrrrr|}
 &$ y $& $\hat{y}$ &$ u $& $\hat{u}$ &  $\hat{\beta}$ \\
\hline
\hline
$ y $\\
$ \hat{y} $\\
$ u $\\
$ \hat{u} $\\
$ \hat{\beta} $\\
$\ y_{test}$ \\
$\hat{y}_{test}$ \\
$\hat{u}_{test}$ \\
\hline
\end{tabular}
\\\\

\textit{Решение}:

Будем пользоваться результатами, полученными ранее:

\[
 \hat\beta=(X^TX)^{-1}X^Ty
\]
\[
 \hat{y}=X\hat\beta
\]
\[
 \hat{u}=y-\hat{y}
 \]
\[
  y_{test}=X_{test}\beta+u_{test}
\]
\[
 \hat{y}_{test}=X_{test}\hat{\beta}
\]

\indent Предварительно найдём математические ожидания:

\[
\E(y|X)=\E(X\beta+u|X)=X\beta+\E(u|X)=X\beta
\]

\[
\E(\beta|X)=\E((X^TX)^{-1}X^Ty|X)=\E((X^TX)^{-1}X^TX\beta|X)=(X^TX)^{-1}X^TX\beta=\beta
\]

\indent Получим элементы мега-матрицы:

\[
\Var(y|X)=\Var(X\beta+u|X)=\Var(u|X)=\sigma^2I\\
\]
\begin{eqnarray*}
\Var(\beta|X)=\Var((X^TX)^{-1}X^Ty|X)=(X^TX)^{-1}X^T\Var(y|X)((X^TX)^{-1}X^T)^T=\\
(X^TX)^{-1}X^T\sigma^2 X(X^TX)^{-1}=\sigma^2 (X^TX)^{-1}X^TX(X^TX)^{-1}= \sigma^2 (X^TX)^{-1}
\end{eqnarray*}
\begin{eqnarray*}
\Cov(\hat{\beta},\hat{u}|X)=\Cov((X^TX)^{-1}X^Ty, y-\hat{y}|X)=
\Cov((X^TX)^{-1}X^Ty,y|X)-\Cov((X^TX)^{-1}X^Ty,\hat{y}|X)=\\
(X^TX)^{-1}X^T\Var(y|X)-\Cov(\beta,X\beta|X)
=\sigma^2(X^TX)^{-1}X^T-\sigma^2(X^TX)^{-1}X^T=0
\end{eqnarray*}

\textit{\textbf{На контрольной работе - вывести 2 элемента мега-матрицы.}}

\subsection{Математическое ожидание RSS}
$RSS$ можно представить как:\\
\[
RSS=\sum\limits_{i=1}^n  \hat{u_i}^2=\hat{u}^T\hat{u}=((I-H)y)^T((I-H)y)=
y^T(I-H)^T(I-H)y=y^T(I-H)y
\]
Далее, найдем математическое ожидание $RSS$ при фиксированном $X$:
\begin{eqnarray*}
\E(RSS|X)=\E(y^T(I-H)y|X)=\E(tr(y^T(I-H)y|X))=\E(tr(I-H)yy^T|X))=tr((I-H)\E(yy^T))=\\
\tr(I-H)(\Var(y|X)+\E(y|X)\E(y^T|X))=tr(I-H)(\sigma^2I+X\beta\beta^TX^T))=tr[(I-H)(\sigma^2I+X\beta\beta^TX^T))]=\\
\tr(I-H)\sigma^2+tr((I-H)(X\beta\beta^TX^T))=tr(I-H)\sigma^2=(n-k)\sigma^2=(n-k)\sigma^2
\end{eqnarray*}
Слагаемое $(I-H)X=0$, так как $(I-H)$ — проекция на ортогональное дополнение к $X$. Можем сделать вывод, что оценка $\hat{\sigma}^2=\frac{RSS}{n-k}$ — несмещённая.

\section{Теорема Гаусса-Маркова}

\begin{theorem}
Если:
\begin{itemize}
    \item Выполняются предпосылки про $\beta$, $u$, $X$;
    \item Помимо МНК-оценки $\hat{\beta}=(X^TX)^{-1}X^Ty$ существует альтернативная несмещённая оценка $\hat{\beta}_{alt}=A_{alt}^Ty$
\end{itemize}
то можно утверждать, что:
\begin{itemize}
    \item $\Var({\hat{\beta}_{alt_j}|X)} \geq{\Var(\hat{\beta}_j|X)}$
    \item Матрица $Delta = \Var({\hat{\beta}_{alt}|X)}-\Var(\hat{\beta}|X)$ являетеся неотрицатиельно положительно определенной.
\end{itemize}
\textit{Замечание.}\\
Первая формулировка является частным и упрощенным вариантом второй формулировки.
\begin{proof}
$\hat{\beta}=((X^TX)^{-1}X^T)^Ty=A^Ty\\$
$\hat{\beta}_1$ — первая строка из $A^T$, умноженная на $y$.
Также $\hat{\beta}_1 = \langle \text{первый столбец в } A,  y\rangle$.
Матрица $A$ задаёт веса, с которыми надо брать $y$, чтобы получить $\hat{\beta}$.
Рассмотрим, каким образом собрана матрица $A$:
\[
A=X\cdot(X^TX)^{-1}
\]
Для этого обозначим столбцы матрицы $A$ векторами $c_1$, $c_1$, \ldots, $c_k$:
\[A=
\begin{pmatrix}
\vdots & \vdots & \dots & \vdots \\
c_1 & c_2 & \dots & c_k\\
\vdots & \vdots & \vdots &\vdots\\
\end{pmatrix}
\cdot
\begin{pmatrix}
* & \dots \\
* & \dots \\
\vdots &\dots\\
* & \dots \\
\end{pmatrix}
\]

Первый столбец матрицы $A$ формируется, как линейная комбинация столбцов $c_i$: \\
$a_1=* \cdot c_1+* \cdot c_2+...+ * \cdot c_k$
\\
Веса, с которыми надо брать $y$, чтобы получить $\hat{\beta}$, являются линейной комбинацией столбцов матрицы $X$.

Дисперсия $\Var(\hat{\beta}_1|X)=\Var(a_1^Ty|X)=a_1^T\sigma^2Ia_1=\sigma^2a_1^Ta_1=\sigma^2\cdot
\begin{Vmatrix}
  \ a_1
\end{Vmatrix}^2$\\
$\Var(\hat{\beta_1}^{alt}|X)=\sigma^2
\begin{Vmatrix}
  \ a_1^{alt}

\end{Vmatrix}^2 \geq \Var(\hat{\beta_1}|X)$\\

Краткая схема доказательства:
\begin{enumerate}
    \item $a_1 \in \Lin(\text{столбцы}\enspace X)$\\
Это следует из $A=X(X^TX)^{-1}$
    \item $a_1^{alt}-a_1 \perp \Lin(\text{столбцы} \enspace X)$\\
Это следует из $(a_1^{alt})^TX\beta=\beta_1=a_1^TX\beta$
\end{enumerate}
Рассмотрим доказательство для общего случая:

Введём матрицу $\Delta=\Var(\hat{\beta}^{alt}|X)-\Var(\hat{\beta}|X)$.
Нам нужно доказать, что матрица $\Delta$ неотрицательно определённая.
Для этого для любого вектора $w$ нужно, чтобы $w^T \Delta w \geq 0$.
Или, эквивалентно:
\[
\Var(\omega^T \hat{\beta}_{alt}|X) \geq \Var(\omega^T \hat{\beta}|X)
\]
\[
\Var(a_{alt}^TY|X) \geq \Var(a^Ty|X)
\]
Далее доказательство, как и для частного случая:
\begin{enumerate}
    \item $a \in \Lin(\text{столбцы} \enspace X)$
    \item $a^T-a \perp \Lin(\text{столбцы} \enspace X)$
\end{enumerate}
\end{proof}
\end{theorem}

\section{Теоремы без доказательства}
Если верны предпосылки о $\beta, X, u,$ и известно, что $u \sim N(0;\sigma^2I)$, то:
\begin{enumerate}
    \item $\hat{\beta}_j|X \sim N(\beta_j, \Var(\hat{\beta}_j|X))$
    \item $\frac{\hat{\beta}_j-\beta_j}{\sqrt{\Var(\hat{\beta}_j|X)}}=\frac{\hat{\beta}_j-\beta_j}{se(\hat{\beta}_j)} \sim t_{n-k}$
    \item Если существуют две вложенные модели: «short» — частный случай «long», то:
    \[
\frac{(RSS_s-RSS_l)/(k_l-k_s)}  {RSS_l/(n-k_l)} \sim F_{k_l-k_s,n-k_l}
\]
\end{enumerate}
\subsection{Задача}
\textit{Дано:} 200 наблюдений.\\
Модель А: $\hat{y}_i=2.7 - 3.2x_i, \hspace{10pt} R^2=0.6$\\
\indent \hspace{2cm} (1.1)\hspace{10pt}(0.6) \\
Модель В: $\hat{y}_i=3.2 - 2.8x_i + 4.2z_i + 0.1x_i^2, \hspace{10pt} R^2=0.7\\
\indent \hspace{2cm}  (1.2) \hspace{10pt} (0.7) \hspace{10pt} (1.1)  \hspace{10pt}  (0.5)$\\
В скобках ниже указаны стандартные ошибки, standard errors, $se(\hat\beta)$ \\
\indent \textit{Задание}:
\begin{enumerate}
    \item Проверьте гипотезу при уровне значимости $\alpha = 5\% $

$H_0$: верна модель А.\\
$H_A$: модель А неверна, но верна модель В.

\item Постройте 95\% доверительный интервал для $\beta_x.$
\end{enumerate}
\textit{Решение}

Из теоремы 2:
\[ \frac{\hat{\beta}_x-\beta_x}{se(\hat{\beta}_x)}\sim t_{200-2}\]
Наблюдений много, поэтому $t$-распределение с 198 степенями свободы очень похоже на нормальное.\\
$t_{198} = N(0;1)$\\
$-t_{cr} \leq \frac{\hat{\beta}_x-\beta_x}{se(\hat{\beta}_x)} \leq t_{cr} $\\
Дробь с вероятностью 95\% лежит от $-1.96$ до $1.96$.
\[
\beta_x \in [\hat \beta_x  - t_{cr} se(\hat{\beta}_x); \hat \beta_x  + t_{cr} se(\hat{\beta}_x)]
\]
\[
\beta_x \in [-3.2 - 1.96\cdot0.6; -3.2 + 1.96\cdot0.6] \approx [-4.4;-2.0]
\]
Замечаем, что $\beta_x= 0$ не попадает в $[-4.4;-2.0]$.\\
$H_0:\beta_x=0$\\
$H_A:\beta \neq 0, при \alpha=5\%$\\
$H_0$ отвергается, $\hat \beta_x$ значительно отличается от нуля.
\[
R^2=1-\frac{RSS}{TSS}
\]
Воспользуемся третьей теоремой:
\[ \frac{\frac{{RSS}_s-RSS_l}{k_l-k_s}}  {\frac{RSS_l}{n-k_l}} \sim F_{k_l-k_s,n-k_l}
\]
\[
\frac{\frac{{RSS}_s-RSS_l}{4-2}}  {\frac{RSS_l}{200-4}}=\frac{\frac{{RSS}_s-RSS_l}{TSS}/2}  {\frac{RSS_l}{TSS}/196}=
\frac{(0.4-0.3)\cdot196}{2\cdot0.3} \approx 3.04
\]
\[
F_{cr} = 3.04
\]
Сравним $F_{cr}$ с $F_{2,196}$, видим, что гипотеза $H_0$ отвергается, поэтому выбираем модель B для оценки.

\end{document}
