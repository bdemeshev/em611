\documentclass[12pt]{article} % размер шрифта
% \usepackage{tikz} % картинки в tikz
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{microtype} % свешивание пунктуации
\usepackage{array} % для столбцов фиксированной ширины
\usepackage{url} % для вставки ссылок \url{...}
\usepackage{indentfirst} % отступ в первом параграфе
\usepackage{sectsty} % для центрирования названий частей
\allsectionsfont{\centering} % приказываем центрировать все sections
\usepackage{amsthm} % теоремы и доказательства
\theoremstyle{definition} % прямой шрифт в условии теорем
\newtheorem{theorem}{Теорема}[section]
\usepackage{amsmath} % куча стандартных математических плюшек
\usepackage[top=2cm, left=1.5cm, right=1.5cm, bottom=2cm]{geometry} % размер текста на странице
\usepackage{lastpage} % чтобы узнать номер последней страницы
\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке
\usepackage{caption} % подписи к картинкам без плавающего окружения figure
\linespread{1.4}

\usepackage{setspace} % для указания длины разрывов

\usepackage{fancyhdr} % весёлые колонтитулы
\pagestyle{fancy}
\lhead{Эконометрика, финтех}
\chead{}
\rhead{2018-10-13, встреча 4}
\lfoot{}
\cfoot{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}



\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет картина Последний день Помпеи}
% команда \listoftodos — печатает все поставленные \todo'шки

\usepackage{booktabs} % красивые таблицы
% заповеди из документации:
% 1. Не используйте вертикальные линии
% 2. Не используйте двойные линии
% 3. Единицы измерения помещайте в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"

\usepackage{fontspec} % поддержка разных шрифтов
\usepackage{polyglossia} % поддержка разных языков
\usepackage{amsfonts}

\setmainlanguage{russian}
\setotherlanguages{english}

\setmainfont{Linux Libertine O} % выбираем шрифт
% если Linux Libertine не установлен, то
% можно также попробовать Helvetica, Arial, Cambria и т.Д.

% чтобы использовать шрифт Linux Libertine на личном компе,
% его надо предварительно скачать по ссылке
% http://www.linuxlibertine.org/index.php?id=91&L=1

% на сервисах типа sharelatex.com этот шрифт есть :)

\newfontfamily{\cyrillicfonttt}{Linux Libertine O}
% пояснение зачем нужно шаманство с \newfontfamily
% http://tex.stackexchange.com/questions/91507/

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
\setlist[enumerate, 2]{label=\asbuk*),ref=\asbuk*} % списки уровня 2 будут буквами а) б) ...

%% эконометрические и вероятностные сокращения
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{E}
\def \hb{\hat{\beta}}
\def \hs{\hat{\sigma}}
\def \htheta{\hat{\theta}}
\def \s{\sigma}
\def \hy{\hat{y}}
\def \hY{\hat{Y}}
\def \v1{\vec{1}}
\def \e{\varepsilon}
\def \he{\hat{\e}}
\def \hu{\hat{u}}
\def \z{z}
\def \hVar{\widehat{\Var}}
\def \hCorr{\widehat{\Corr}}
\def \hCov{\widehat{\Cov}}
\def \cN{\mathcal{N}}


\begin{document}
Конспектировал: Ефим Лубошников
\section{Свойства математического ожидания}
Вспомним, какими свойствами обладает математическое ожидание случайной величины и случайного вектора.
Обозначим:
$y_1$, $y_2$ - случайные величины,
$y$, $z$ - случайные вектора,
$a$ - скалярная константа,
$A$, $B$ - матрицы констант.

\begin{center}
\begin{tabular}{lll}
\toprule
Случайная величина & Случайный вектор  \\
\midrule
${E}(ay_1)=a{E}(y_1)$ & ${E}(AyB)=A{E}(y)B$\\
${Var}(y_1)={E}(y_1^2)-{E}(y_1)^2$ & ${Var}(y)={E}(yy^T)-{E}(y){E}(y^T)$\\
${Var}(ay_1)=a^2{Var}(ay_1)$ & ${Var}(Ay)=A({E}(yy^T)-{E}(y){E}(y^T))A^T$ \\
${Var}(a+y_1)={Var}(y_1)$& ${Var}(a+y)={Var}(y)$ \\
${Cov}(y_1,y_2)={E}(y_1y_2)-{E}(y_1){E}(y_2)$ & ${Cov}(y,z)={E}(yz^T)-{E}(y){E}(z^T)$ \\
 & ${Cov}(Ay,Bz)=A{Cov}(y,z)B^T$ \\
 & ${Cov}(y,z)={Cov}(z,y)^T$ \\
 &Если $ y,z \in \mathbb {R}^n $, то справедливо:\\
 & ${Cov}(y+z,w)={Cov}(y,w)+{Cov}(z,w)$ \\
 & ${Var}(y+z)={Var}(y)+{Var}(z)+{Cov}(y,z)+{Cov}(z,y),$\\
\bottomrule
\end{tabular}
\end{center}

Так выглядит дисперсия случайного вектора $y \in \mathbb {R}^n $:
\[
{Var}(y) = \begin{bmatrix}
           {Var}(y_{1}) & {Cov}(y_{1},y_{2}) & \vdots &{Cov}(y_{1},y_{n})\\
           {Cov}(y_{2},y_{1}) & {Var}(y_{2}) & \vdots &{Cov}(y_{2},y_{n})\\
           \vdots & \vdots & \vdots & \vdots\\
           {Cov}(y_{n},y_{1}) & {Cov}(y_{n},y_{2}) & \vdots &{Var}(y_{n})\\
         \end{bmatrix}
         \]
\section{Предпосылки для теоремы Гаусса-Маркова}
Рассмотрим задачу регрессии $y=X\beta+u$ в случае двух регрессоров: $y_i=\beta_1+\beta_2x_i+\beta_3z_i+u_i$
Предпосылки:
\begin{enumerate}
    \item Предполагаем $\beta$ -неизвестная константа
    \item $X$ должен иметь полный ранг, чтобы оценки МНК существовали. $X$ может быть:
    \begin{enumerate}
        \item известная константа, $y_t=\beta_1+\beta_2t+u_t$
        \item наблюдение - случайная величина, времянной ряд.
    \end{enumerate}
    \item Гомоскедастичность ${Var}(u_i|X)=\sigma^2I$
    \item Отсутствие автокорреляции
    ${Cov}(u_i,u_j|x)=0, при i\neq j$
    \item$ {E}(u|X)=0$
\end{enumerate}
Обозначим:\\
    $\hat{y}$  -  предсказанный $y$ на тренировочной выборке.\\
    $\hat{y}_{test}$  -  предсказанный $y$ на тестовой выборке.
\newpage
\begin{doublespacing}




\subsection{Упражнение мега-матрица}
\textit{Задание}: Заполнить матрицу ${Var}$, ${Cov}$ для соответсвующих элементов матрицы:\\\\
\begin{tabular}{c||rrrrrr|}
 &$ y $& $\hat{y}$ &$ u $& $\hat{u}$ &  $\hat{\beta}$ \\
\hline
\hline
$ y $\\
$ \hat{y} $\\
$ u $\\
$ \hat{u} $\\
$ \hat{\beta} $\\
$\ y_{test}$ \\
$\hat{y}_{test}$ \\
$\hat{u}_{test}$ \\

\hline
\end{tabular}\\\\
\textit{Решение}:
\begin{doublespacing}

Будем пользоваться результатами, полученными ранее:\\
 $\hat\beta=(X^TX)^{-1}X^Ty$\\
 $\hat{y}=X\hat\beta$\\
 $\hat{u}=y-\hat{y}$\\
 $ y_{test}=X_{test}\beta+u_{test}$\\
 $\hat{y}_{test}=X_{test}\hat{\beta}$\\
\end{doublespacing}
\indent Предварительно найдём мат.ожидания:\\
\noindent ${E}(y|X)={E}(X\beta+u|X)=X\beta+{E}(u|X)=X\beta$\\
${E}(\beta|X)={E}((X^TX)^{-1}X^Ty|X)={E}((X^TX)^{-1}X^TX\beta|X)=(X^TX)^{-1}X^TX\beta=\beta$\\
\indent Получим элементы мега-матрицы:\\
${Var}(y|X)={Var}(X\beta+u|X)={Var}(u|X)=\sigma^2I$\\
${Var}(\beta|X)={Var}((X^TX)^{-1}X^Ty|X)=(X^TX)^{-1}X^T{Var}(y|X)((X^TX)^{-1}X^T)^T=(X^TX)^{-1}X^T\sigma^2 X(X^TX)^{-1}=\sigma^2 (X^TX)^{-1}X^TX(X^TX)^{-1}= \sigma^2 (X^TX)^{-1} $\\
${Cov}(\hat{\beta},\hat{u}|X)={Cov}((X^TX)^{-1}X^Ty, y-\hat{y}|X)=
{Cov}((X^TX)^{-1}X^Ty,y|X)-{Cov}((X^TX)^{-1}X^Ty,\hat{y}|X)=(X^TX)^{-1}X^T{Var}(y|X)-{Cov}(\beta,X\beta|X)=\sigma^2(X^TX)^{-1}X^T-\sigma^2(X^TX)^{-1}X^T=0$\\
\textit{\textbf{На контрольной работе - вывести 2 элемента мега-матрицы.}}
\end{doublespacing}

Выведем ещё пару свойств для $RSS, ESS, TSS$:\\
Как можно представить RSS?\\
$RSS=\sum\limits_{i=1}^n  \hat{u_i}^2=\hat{u}^T\hat{u}=((I-H)y)^T((I-H)y)=
y^T(I-H)^T(I-H)y=y^T(I-H)y$\\
Далее, найдем математическое ожидание $RSS$ при фиксированном $X$:\\
${E}(RSS|X)={E}(y^T(I-H)y|X)=|$всегда можно взять $trace$, если результат скаляр$|={E}(tr(y^T(I-H)y|X))={E}(tr(I-H)yy^T|X))=tr((I-H){E}(yy^T))=tr(I-H)({Var}(y|X)+{E}(y|X){E}(y^T|X))=tr(I-H)(\sigma^2I+X\beta\beta^TX^T))=tr[(I-H)(\sigma^2I+X\beta\beta^TX^T))]=tr(I-H)\sigma^2+tr((I-H)(X\beta\beta^TX^T))=tr(I-H)\sigma^2=(n-k)\sigma^2$\\
Слагаемое $(I-H)X=0$, так как $(I-H)$ - ортогональное дополнение к $X$.\\
${{E}(RSS|X)=(n-k)\sigma^2}\\ \Rightarrow
{\hat{\sigma}^2={{RSS}\over{n-k}}}$ - оценка разумная.

\subsection{Теорема Гаусса - Маркова}
\begin{theorem}
Если выполняются все предпосылки про $\beta,u,X$, существуют несмещенные оценки  $\hat{\beta}=(X^TX)^{-1}X^Ty$  и  $\hat{\beta}_{alt}=A_{alt}^Ty,  $  то:\\
\indent ${Var}({\hat{\beta}_{alt_j})} \geq{{Var}(\hat{\beta}_j|X)}$ \\
\indent ${Var}({\hat{\beta}_{alt})}-{Var}(\hat{\beta}|X)$ являетеся положительно определенной.
\end{theorem}

\textit{Замечание.} Первая формулировка является частным и упрощенным вариантом второй формулировки.

\begin{proof}
$\hat{\beta}=((X^TX)^{-1}X^T)^Ty=A^Ty\\$
$\hat{\beta}_1$ - 1 строка из $A^T$. Также
$\hat{\beta_1}$ - <первый столбец в $A$,  y>
$A$ задаёт веса, с которыми надо брать $y$, чтобы получить $\hat{\beta}$.\\
Рассмотрим, каким образом собрана матрица $A$:\\


$\begin{pmatrix}
C_1 & C_2 & \dots &C_k\\
C_1 & C_2 & \dots & C_k\\
\vdots & \vdots & \vdots &\vdots\\
C_1 & C_2 & \dots & C_k
\end{pmatrix}$

$\begin{pmatrix}
* & \dots \\
* & \dots \\
\vdots &\dots\\
* & \dots \\
\end{pmatrix}$

Первая строка матрицы $A$ формируется, как линейная комбинация $C_i$: \\
$a_1=*C_1+*C_2+...+*C_k$


Веса, с которыми надо барть $y$, чтобы получить $\beta$ являются линейной комбинацией строк в $X$.


Дисперсия ${Var}(\hat{\beta}_1|X)={Var}(a_1^Ty|X)=a_1^T\sigma^2Ia_1=\sigma^2a_1^Ta_1=\sigma^2*
\begin{Vmatrix}
  \ a_1
\end{Vmatrix}^2$

${Var}(\hat{\beta_1}^{alt}|X)=\sigma^2
\begin{Vmatrix}
  \ a_1
\end{Vmatrix}^2 \geq {Var}(\hat{\beta_1}|X)$

Краткая схема доказательства:\\
1)$a_1 \in lin(...)$\\
Это следует из $A=X(X^TX)^{-1}$ \\
2)$a_1^{alt}-a_1 \perp lin(...) $\\
Это следует из $(a_1^{alt})^TX\beta=\beta_1=a_1^TX\beta$\\
Рассмотрим доказательство для общего случая:\\
$\delta={Var}(\hat{\beta}^{alt}|X)-{Var}(\hat{\beta}|X)\\
\delta - $ положительно определённая.\\
$\omega^T\omega \geq 0 \\
{Var}(\omega^T \hat{\beta}_{alt}|X) \geq {Var}(\omega^T \hat{\beta}|X)\\
{Var}(a_{alt}^TY|X) \geq {Var}(a^Ty|X)\\
1) a \in lin(столбцы X);\\
2) a^T-a \perp lin(стоблцы X)$\\
\end{proof}


\textit{Теоремы без доказательства}

Если верны предпосылки о $\beta, X, u,$ и известно, что $u \sim N(0;\sigma^2I)$, то: \\
\indent 1) $\hat{\beta}_j|X \sim N(\beta_j, {Var}(\hat{\beta}_j|X))$
\\
\indent 2) $\frac{\hat{\beta}_j-\beta_j}{{Var}(\hat{\beta}_j|X)}=\frac{\hat{\beta}_j-\beta_j}{\kappa(\hat{\beta}_j)} \sim t_{n-k}$
\\
\indent 3) Если $\exists$ 2 вложенные модели: "short" - частный случай "long", то:\\
$$ \frac{\frac{{RSS}_s-RSS_l}{k_l-k_s}}  {\frac{RSS_l}{n-k_l}} \sim F_{k_l-k_s,n-k_l}$$

\textbf{\textit{Задача}}\\
\textit{Дано:} 200 наблюдений.\\
Модель А: $\hat{y}_i=2.7 - 3.2x_i, \hspace{10pt} R^2=0.6$\\
\indent \hspace{2cm} (1.1)\hspace{10pt}(0.6) \\
Модель В: $\hat{y}_i=3.2 - 2.8x_i + 4.2z_i + 0.1x_i^2, \hspace{10pt} R^2=0.7\\
\indent \hspace{2cm}  (1.2) \hspace{10pt} (0.7) \hspace{10pt} (1.1)  \hspace{10pt}  (0.5)$\\
В скобках ниже указаны стандартные ошибки\\
\textit{Задание}:
1. Проверить гипотезу при уровне значимости $\alpha = 5\% $  \\
$H_0$: верна модель А.\\
$H_A$: модель А - неверна, но верна В.

\noindent 2.Постройте 95\% доверительный интервал для $\beta_x.$\\
\textit{Решение}\\Из теоремы 2 :\\
$$ \frac{\hat{\beta}_x-\beta_x}{\kappa(\hat{\beta}_x)}\sim t_{200-2}$$
T-распределение с 198 степенями свободы очень похоже на нормальное.\\
$t_{198} = N(0;1)$\\
$-t_{cr} \leq \frac{\hat{\beta}_x-\beta_x}{\kappa(\hat{\beta}_x)} \leq t_{cr} $\\
Дробь с вероятностью 95\% лежит от -1.96 до 1.96.
$\beta_x \in [\hat{\beta_x} - t_{cr} \kappa(\hat{\beta}_x); \hat{\beta_x} + t_{cr} \kappa(\hat{\beta}_x)]$

$\beta_x \in [-3.2 - 1.96*0.6; -3.2 + 1.96*0.6] \approx [-4.4;2.0]$\\
Предположим, $\beta_x$=0, попадает в $[-4.4;2.0]$.\\
$H_0:\beta_x=0$\\
$H_A:\beta \neq 0, при \alpha=5\%$\\
$H_0$ отвергается, $\beta_x$ значительно отличается от нуля.\\
$$R^2=1-\frac{RSS}{TSS}$$\\
Воспользуемся 3 теоремой:\\
$$ \frac{\frac{{RSS}_s-RSS_l}{k_l-k_s}}  {\frac{RSS_l}{n-k_l}} \sim F_{k_l-k_s,n-k_l}$$

$$ \frac{\frac{{RSS}_s-RSS_l}{4-2}}  {\frac{RSS_l}{200-196}}=\frac{\frac{{RSS}_s-RSS_l}{TSS}/2}  {\frac{RSS_l}{TSS}/196}=
\frac{(0.4-0.3)*196}{2*0.3} \approx 3.04$$\\
$F_{cr} = 3.04$\\
Сравним $F_{cr}$ с $F_{2,196}$, видим, что гипотеза $H_0$ отвергается, поэтому выбираем модель B для оценки.


























\end{document}
